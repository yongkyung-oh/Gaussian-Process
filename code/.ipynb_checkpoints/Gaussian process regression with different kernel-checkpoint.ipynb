{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process for Machine Learning \n",
    "## Gaussian prior and posterior\n",
    "\n",
    "- Implementation of \"Seeger, M. (2004). Gaussian processes for machine learning. International journal of neural systems, 14(02), 69-106.\"\n",
    "- Equations and approaches are cited from \"Bishop, C. M. (2006). Pattern recognition and machine learning. springer.\"\n",
    "- Using **GPyTorch** module to test sample function with different kernels: Gardner, Jacob R., Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wilson. ” GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration.” In NeurIPS (2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply Gaussian process models to the problem of regression, we need to take account of the noise on the observed target values\n",
    "\n",
    "$$y_n = u_n + \\epsilon_n$$\n",
    "\n",
    "where $u_n = u(x_n)$ and $\\epsilon_n$ is random noise variable whose value is chosen independently for each observation $n$. Assume that we observe $N$ data points in the $N$-dimensional space.\n",
    "\n",
    "Here we shall consider noise processes that have a Gaussian distribution, so that\n",
    "\n",
    "$$p(y_n|u_n) = \\mathcal{N}(y_n|u_n, \\beta^{-1})$$\n",
    "\n",
    "where $\\beta$ is a hyperparameter representing the precision of the noise.\n",
    "\n",
    "For indenpendent data points, the joint distribution of the target values $\\mathbf{y} = (y_1, \\dots, y_N)^T$ conditioned on $\\mathbf{u} = (u_1, \\dots, u_N)^T$ is given by an isotropic Gaussian of the form\n",
    "\n",
    "$$p(\\mathbf{y}|\\mathbf{u}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{u}, \\beta^{-1}\\mathbf{I}_N)$$\n",
    "\n",
    "where $\\mathbf{I}_N$ denotes the N ×N unit matrix. From the definition of a Gaussian process, the marginal distribution $p(\\mathbf{u})$ is given by a Gaussian whose mean is zero and whose covariance is defined by a Gram matrix $\\mathbf(K)$ so that\n",
    "\n",
    "$$p(\\mathbf{u})=\\mathcal{N}(\\mathbf{u}|0,\\mathbf{K})$$\n",
    "\n",
    "The kernel function that determines $\\mathbf{K}$ is typically chosen to express the property that, for points $x_n$ and $x_m$ that are similar, the corresponding values $u(x_n)$ and $u(x_m)$ will be more strongly correlated than for dissimilar points.\n",
    "\n",
    "The marginal distribution of $\\mathbf{y}$ is given by\n",
    "\n",
    "$$p(\\mathbf{y}) = \\int{p(\\mathbf{y}|\\mathbf{u})p(\\mathbf{u})d\\mathbf{u}} = \\mathcal{N}(\\mathbf{y}|0,\\mathbf{C})$$\n",
    "\n",
    "where the covariance matrix $\\mathbf{C}$ has elements\n",
    "\n",
    "$$C(x_n,x_m)=k(x_n,x_m)+\\beta^{-1}\\delta_{nm}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution over $y_1, \\dots, y_{N+1}$ will be given by\n",
    "\n",
    "$$p(\\mathbf{y}_{N+1}) = \\mathcal{N}(\\mathbf{y}_{N+1}|0,\\mathbf{C}_{N+1})$$\n",
    "\n",
    "Because this joint distribution is Gaussian, we can apply the conditional Gaussian distribution.\n",
    "\n",
    "$$\\mathbf{C}_{N+1} = \\left(   \n",
    "                     \\begin{matrix}\n",
    "                     \\mathbf{C}_{N} & \\mathbf{k} \\\\\n",
    "                     \\mathbf{k} & c \\\\\n",
    "                     \\end{matrix}\n",
    "                     \\right)$$\n",
    "\n",
    "where $c=k(x_{N+1}, x_{N+1})+\\beta^{-1}$. We see that the conditional distribution $p(y_{N+1}|\\mathbf{y})$ is a Gaussian distribution with mean and covariance given by \n",
    "\n",
    "$$\\mu(x_{N+1}) = \\mathbf{K}^T \\mathbf{C}^{-1}_N \\mathbf{y}$$\n",
    "$$\\sigma^2(x_{N+1}) = c-\\mathbf{K}^T \\mathbf{C}^{-1}_N \\mathbf{y}$$\n",
    "\n",
    "Instead of regressing against some known function, lets just see what happens when we predict based on the sampled prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gpytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d9663adf2166>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gpytorch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes we have a simple function of $x$ to generate $y$. \n",
    "\n",
    "\n",
    "$$y = \\sin(2\\pi x) + \\epsilon $$\n",
    "$$\\epsilon \\sim \\mathcal{N}(0, 0.5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "\n",
    "# Prior train data is in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 50)\n",
    "\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "true_y = torch.sin(train_x * (2 * math.pi))\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    ax.plot(train_x.numpy(), true_y.numpy())\n",
    "    ax.plot(train_x.numpy(), train_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class RBF_GP_Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(RBF_GP_Model, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(lengthscale=1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class Matern_GP_Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(Matern_GP_Model, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)    \n",
    "\n",
    "class Linear_GP_Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(Linear_GP_Model, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class Periodic_GP_Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(Periodic_GP_Model, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)    \n",
    "\n",
    "class Polynomial_GP_Model(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(Polynomial_GP_Model, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PolynomialKernel(power=4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_x = torch.randn(100)\n",
    "random_y = torch.randn(100)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "model = RBF_GP_Model(random_x, random_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 100)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f0, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    \n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "model = RBF_GP_Model(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 100)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f1, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    \n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 100\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "#    if i%10 ==9:\n",
    "#        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "#            i + 1, training_iter, loss.item(),\n",
    "#            model.covar_module.base_kernel.lengthscale.item(),\n",
    "#            model.likelihood.noise.item()\n",
    "#        ))\n",
    "    optimizer.step()\n",
    "print(\"\\n\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 100)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f2, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    \n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f0.savefig('result/prior-RBF_GP_Model.png')\n",
    "#f1.savefig('result/posterior-RBF_GP_Model.png')\n",
    "#f2.savefig('result/posterior(trained)-RBF_GP_Model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "#### RBF kernel \n",
    "\\begin{equation*}\n",
    "   k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}\n",
    "   (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-1} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "| Prior | Posterior | Posterior (Trained) |\n",
    "|:------:|:------:|:------:|\n",
    "| ![img](result/prior-RBF_GP_Model.png) | ![img](result/posterior-RBF_GP_Model.png) | ![img](result/posterior(trained)-RBF_GP_Model.png) |\n",
    "\n",
    "\n",
    "#### Matern kernel\n",
    "\\begin{equation*}\n",
    "   k_{\\text{Matern}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)}\n",
    "   \\left( \\sqrt{2 \\nu} d \\right) K_\\nu \\left( \\sqrt{2 \\nu} d \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Where $d = (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-1} (\\mathbf{x_1} - \\mathbf{x_2})$, $\\nu$ is a smoothness parameter and $K_\\nu$ is a modified Bessel function.\n",
    "\n",
    "\n",
    "$$\\nu=0.5$$\n",
    "\n",
    "| Prior | Posterior | Posterior (Trained) |\n",
    "|:------:|:------:|:------:|\n",
    "| ![img](result/prior-Matern_GP_Model-0.5.png) | ![img](result/posterior-Matern_GP_Model-0.5.png) | ![img](result/posterior(trained)-Matern_GP_Model-0.5.png) |\n",
    "\n",
    "$$\\nu=1.5$$\n",
    "\n",
    "| Prior | Posterior | Posterior (Trained) |\n",
    "|:------:|:------:|:------:|\n",
    "| ![img](result/prior-Matern_GP_Model-1.5.png) | ![img](result/posterior-Matern_GP_Model-1.5.png) | ![img](result/posterior(trained)-Matern_GP_Model-1.5.png) |\n",
    "\n",
    "$$\\nu=2.5$$\n",
    "\n",
    "| Prior | Posterior | Posterior (Trained) |\n",
    "|:------:|:------:|:------:|\n",
    "| ![img](result/prior-Matern_GP_Model-2.5.png) | ![img](result/posterior-Matern_GP_Model-2.5.png) | ![img](result/posterior(trained)-Matern_GP_Model-2.5.png) |\n",
    "\n",
    "\n",
    "#### Linear kernel\n",
    "\\begin{equation*}\n",
    "    k_\\text{Linear}(\\mathbf{x_1}, \\mathbf{x_2}) = v\\mathbf{x_1}^\\top\n",
    "    \\mathbf{x_2}\n",
    "\\end{equation*}\n",
    "\n",
    "| Prior | Posterior | Posterior (Trained) |\n",
    "|:------:|:------:|:------:|\n",
    "| ![img](result/prior-Linear_GP_Model.png) | ![img](result/posterior-Linear_GP_Model.png) | ![img](result/posterior(trained)-Linear_GP_Model.png) |\n",
    "\n",
    "#### Periodic kernel\n",
    "\\begin{equation*}\n",
    "   k_{\\text{Periodic}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left(\n",
    "     \\frac{2 \\sin^2 \\left( \\pi \\Vert \\mathbf{x_1} - \\mathbf{x_2} \\Vert_1 / p \\right) }\n",
    "     { \\ell^2 } \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "| Prior | Posterior | Posterior (Trained) |\n",
    "|:------:|:------:|:------:|\n",
    "| ![img](result/prior-Periodic_GP_Model.png) | ![img](result/posterior-Periodic_GP_Model.png) | ![img](result/posterior(trained)-Periodic_GP_Model.png) |\n",
    "\n",
    "#### Polynomial kernel\n",
    "\\begin{equation*}\n",
    "    k_\\text{Poly}(\\mathbf{x_1}, \\mathbf{x_2}) = (\\mathbf{x_1}^\\top\n",
    "    \\mathbf{x_2} + c)^{d}.\n",
    "\\end{equation*}\n",
    "\n",
    "$$\\text{Quadratic case: } d=4$$\n",
    "\n",
    "| Prior | Posterior | Posterior (Trained) |\n",
    "|:------:|:------:|:------:|\n",
    "| ![img](result/prior-Polynomial_GP_Model.png) | ![img](result/posterior-Polynomial_GP_Model.png) | ![img](result/posterior(trained)-Polynomial_GP_Model.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Kernel | Marginal Log Likelihood |\n",
    "|:------:|:------:|\n",
    "| RBF | 0.94785737 |\n",
    "| Matern (0.5) | 0.99038773 |\n",
    "| Matern (1.5) | 0.96494627 |\n",
    "| Matern (2.5) | 0.95786011 |\n",
    "| Linear | 1.22133493 |\n",
    "| Periodic | 0.95173829 |\n",
    "| Polynomial (quadratic) | 1.19192600 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### references\n",
    "- Bishop's Pattern Recognition and Machine Learning: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf\n",
    "- Basic code: https://gpytorch.readthedocs.io\n",
    "- Gaussian Distribution: http://norman3.github.io/prml/docs/chapter02/0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
